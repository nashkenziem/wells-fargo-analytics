{"name":"Wells Fargo Analytics Competition","tagline":"","body":"# Wells Fargo Analytics Competition\r\nWells Fargo asked college students from 13 different colleges and universities, selected by the bank, to analyze financial and bank related discussion through two different social media platforms. Wells Fargo provided a data set including tweets from Twitter and comments and posts from Facebook. My colleagues and I cleaned the data and created visuals based on term frequency and sentiment analysis of the data. We then used these visuals to represent attitudes towards the separate banks given by the consumers.\r\n\r\n# Process \r\n**All code in R**\r\n\r\n### Creating a Data Frame\r\nWe created a data frame to create easily accessible data tables. These data tables make it easy to select an entire column or row of data.\r\n```R\r\ndf = read.table('dataset.txt',sep=\"|\",header=T)\r\nspellDoc(doc, wordHandler = DocSpeller(), checker = docChecker(speller), speller = getSpeller(conf), conf = createSpellConfig())\r\n```\r\n### Remove Non-ASCII Characters\r\nASCII characters are alphanumeric characters that have a numeric value that the computer. We removed all non-ASCII characters because the computer cannot process them.\r\n```R\r\ndf.texts.clean = as.data.frame(iconv(df.texts$FullText, \"latin1\", \"ASCII\", sub=\"\"))\r\nfor(i in 1:nrow(df))\r\n{\r\n  df.texts.clean[i,] = check_spelling(df.texts.clean[i,], range = 2, assume.first.correct=TRUE, \r\n                                      dictionary=qdapDictionaries::GradyAugmented,parallel=TRUE,cores=parallel::detectCores()/2,n.suggests=8)\r\n}\r\ncolnames(df.texts.clean) = 'FullText'\r\n```\r\n\r\n### Load Using TM Library\r\nThe TM library is a text mining package downloaded from CRAN. This package contains various functions used to analyze text data.\r\n```R\r\nlibrary(tm) \r\ndocs <- Corpus(DataframeSource(df.texts.clean))\r\n```\r\n\r\n###Creating a Corpus\r\nA corpus is simply a body of text. We consolidated all of the text from the data set into one corpus.\r\n```R\r\ndf.texts = as.data.frame(df[,ncol(df)])\r\ncolnames(df.texts) = 'FullText'\r\n```\r\n\r\n### Cleaning the Text\r\nWe then cleaned the text by removing white spaces, and stop words. Stop words are words like \"the,\" and \"but\" that have no value analytically. We also removed words that Wells Fargo added to protect the anonymity of consumers such as \"Name.\"\r\n```R\r\nremoveNumPunct <- function(x) gsub(\"[^[:alpha:][:space:]]*\", \"\", x)\r\ndocs <- tm_map(docs, content_transformer(removeNumPunct))\r\nmyStopwords <- c(stopwords(kind='SMART'), \"available\", \"via\", \"twithndl\",\"twithndlBankA\",\"twithndlBankB\",\"twithndlBankC\",\r\n                 \"twithndlBankD\",\"twithndlBankE\",\"INTERNET\",\"Name\",\"PHONE\",\"ADDRESS\",\"rettwit\",\"Nameresp\",\"ly\", \"https\", \"bit\",\r\n                 \"dlvr\")\r\ndocs <- tm_map(docs, removeWords, myStopwords, lazy = T)\r\ndocs <- tm_map(docs, stripWhitespace)\r\ndocsCopy <- docs\r\ndocs <- tm_map(docs, stemDocument)\r\ndocs <- Corpus(VectorSource(docs))\r\n```\r\n\r\n### Creating Corpora and Term Document Matricies\r\nWe created individual corpora for each bank then individual term document matrices for each bank as well.\r\nCorpus for Bank A:\r\n```R\r\nbankA.idx = which(sapply(df$FullText,function(x) grepl(\"BankA\",x)))\r\nbankA.docs = docs[bankA.idx]\r\nbankA.docs <- tm_map(bankA.docs, removeWords, c(\"BankA\"), lazy = T)\r\n```\r\nTDM for Bank A\r\n```R\r\ntdmA <- TermDocumentMatrix(bankA.docs, control = list(wordLengths = c(1, Inf)))\r\n```\r\n\r\n### Spell Check\r\nTo check the spelling we used the list of frequent terms for each bank, and the entire data set, and picked out misspelled words and replaced them manually. \r\n```R\r\nterms = dimnames(tdm)$Terms\r\nfor (i in 1:length(terms)) {\r\n\tif (terms[i] == 'appli') {\r\n\t\tterms[i] = 'application'\r\n\t}else if (terms[i] == 'busi') {\r\n\t\tterms[i] = 'business'\r\n\t}else if (terms[i] == 'https') {\r\n\t\tterms[i] = ''\r\n\t}else if (terms[i] == 'manag') {\r\n\t\tterms[i] = 'manage'\r\n\t}else if (terms[i] == 'servic') {\r\n\t\tterms[i] = 'service'\r\n\t}else if (terms[i] == 'advis') {\r\n\t\tterms[i] = 'advise'\r\n\t}else if (terms[i] == 'mortgag') {\r\n\t\tterms[i] = 'mortgage'\r\n\t}else if (terms[i] == 'reiter') {\r\n\t\tterms[i] = 'reiterate'\r\n\t}else if (terms[i] == 'ift') {\r\n\t\tterms[i] = ''\r\n\t}else if (terms[i] == 'tt') {\r\n\t\tterms[i] = ''\r\n\t}else if (terms[i] == 'financi') {\r\n\t\tterms[i] = 'financial'\r\n\t}else if (terms[i] == 'fb') {\r\n\t\tterms[i] = ''\r\n\t}else if (terms[i] == 'compani') {\r\n\t\tterms[i] = 'companies'\r\n\t}else if (terms[i] == 'bank') {\r\n\t\tterms[i] = ''\r\n\t}else if (terms[i] == 'charg') {\r\n\t\tterms[i] = 'charge'\r\n\t}else if (terms[i] == 'peopl') {\r\n\t\tterms[i] = 'people'\r\n\t}else if (terms[i] == 'rais') {\r\n\t\tterms[i] = 'raise'\r\n\t}else if (terms[i] == 'nameresp') {\r\n\t\tterms[i] = ''\r\n\t}else if (terms[i] == 'dirmsg') {\r\n\t\tterms[i] = ''\r\n\t}else if (terms[i] == 'fuck') {\r\n\t\tterms[i] = 'f***'\r\n\t}else if (terms[i] == 'happi') {\r\n\t\tterms[i] = 'happiness'\r\n\t}\r\n}\r\ndimnames(tdm)$Terms = terms\r\n```\r\n\r\n### Visuals Based on Term Frequency \r\nUsing the most frequent terms for the data set we created both a term frequency graph and a word cloud. We also did this for each bank.\r\nTerm Frequency Graph:\r\n```R\r\nlibrary(slam)\r\ndocs <- rollup(docs, 2, na.rm=TRUE, FUN = sum)\r\nfreq.terms <- findFreqTerms(tdm, lowfreq = 1000))      \r\nterm.freq <- rowSums(as.matrix(tdm))\r\nterm.freq <- subset(term.freq, term.freq >= 1000)\r\ntdm <- rollup(tdm, 2, na.rm=TRUE, FUN = sum)\r\nterm.freq <- rowSums(as.matrix(tdm))\r\nterm.freq <- subset(term.freq, term.freq >= 1000)\r\ndf.all <- data.frame(term = names(term.freq), freq = term.freq)\r\nlibrary(ggplot2)\r\nggplot(df.all, aes(x=term, y=freq)) + geom_bar(stat = \"identity\") + xlab(\"Terms\") + ylab(\"Count\") + coord_flip()\r\n```\r\n<a href=\"http://imgur.com/9ChJmcD\"><img src=\"http://i.imgur.com/9ChJmcD.png\" title=\"source: imgur.com\" /></a>\r\n\r\nWord Cloud:\r\n```R\r\nm <- as.matrix(tdm)\r\nword.freq <- sort(rowSums(m), decreasing = T)\r\npal <- brewer.pal(9, \"BuGn\")\r\npal <- pal[-(1:4)]\r\nlibrary(wordcloud)\r\nwordcloud(words = names(word.freq), freq = word.freq, min.freq=2500, random.order=F, random.color=F, colors = pal)\r\n```\r\n<a href=\"http://imgur.com/ULops3z\"><img src=\"http://i.imgur.com/ULops3z.png\" title=\"source: imgur.com\" /></a> \r\n\r\n### Sentiment Analysis\r\npos neg connotation for each tweet\r\n```R\r\npos <- scan('positive-words.txt',what='character',comment.char=';')\r\nneg <- scan('negative-words.txt',what='character',comment.char=';')\r\n\r\nscore.sentiment = function(sentences, pos.words, neg.words, .progress='none')\r\n{\r\n  require(plyr)\r\n  require(stringr)\r\n  \r\n  scores = laply(sentences, function(sentence, pos.words, neg.words) {\r\n    \r\n    # split into words. str_split is in the stringr package\r\n    word.list = str_split(sentence, '\\\\s+')\r\n    # sometimes a list() is one level of hierarchy too much\r\n    words = unlist(word.list)\r\n    \r\n    # compare our words to the dictionaries of positive & negative terms\r\n    pos.matches = match(words, pos.words)\r\n    neg.matches = match(words, neg.words)\r\n    \r\n    # match() returns the position of the matched term or NA\r\n    # we just want a TRUE/FALSE:\r\n    pos.matches = !is.na(pos.matches)\r\n    neg.matches = !is.na(neg.matches)\r\n    \r\n    # and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():\r\n    score = sum(pos.matches) - sum(neg.matches)\r\n    \r\n    return(score)\r\n  }, pos.words, neg.words, .progress=.progress )\r\n  \r\n  scores.df = data.frame(score=scores, text=sentences)\r\n  return(scores.df)\r\n}\r\n```\r\n\r\n### Visuals Based on Sentiment Analysis\r\nWe chose to use a bar plot and a box plot to represent sentiment scores for our data set. The box plot plots a point for the sentiment of each tweet or post separated by platform. The bar plot give the average score for each tweet or post. This is also separated by each platform. \r\nBoxplot:\r\n```R\r\n# global score\r\nglobal_score = round( 100 * numpos / (numpos + numneg) )\r\nscores$mediatype = df.sentiment$MediaType\r\n# colors\r\ncols = c(\"#7CAE00\", \"#00BFC4\")\r\nnames(cols) = c(\"twitter\", \"facebook\")\r\n# boxplot\r\nlibrary(ggplot2)\r\nggplot(scores, aes(x=mediatype, y=score, group=mediatype)) +\r\n  geom_boxplot(aes(fill=mediatype)) +\r\n  scale_fill_manual(values=cols) +\r\n  geom_jitter(colour=\"gray40\",position=position_jitter(width=0.2), alpha=0.3) +\r\n  labs(title = \"Media Type's Sentiment Scores\") + \r\n  xlab('Media Type') + ylab('Sentiment Score')\r\n```\r\n<a href=\"http://imgur.com/T5fq2Uz\"><img src=\"http://i.imgur.com/T5fq2Uz.png\" title=\"source: imgur.com\" /></a>\r\n\r\nBarplot:\r\n```R\r\nmeanscore = tapply(scores$score, scores$mediatype, mean)\r\ndf.plot = data.frame(mediatype=names(meanscore), meanscore=meanscore)\r\ndf.plot$mediatypes <- reorder(df.plot$mediatype, df.plot$meanscore)\r\nggplot(df.plot, aes(x = factor(mediatypes), y = meanscore, fill=mediatypes)) +\r\n  geom_bar(stat=\"identity\") +\r\n  scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n  labs(title = \"Average Sentiment Score\") + \r\n  xlab('Media Type') + ylab('Average Score')\r\n```\r\n<a href=\"http://imgur.com/Rx1WNRJ\"><img src=\"http://i.imgur.com/Rx1WNRJ.png\" title=\"source: imgur.com\" /></a> \r\n\r\n# Visuals for Individual Banks\r\n### Bank A\r\n<a href=\"http://imgur.com/LFIROcV\"><img src=\"http://i.imgur.com/LFIROcV.png\" title=\"source: imgur.com\" /></a>\r\n\r\n### Bank B\r\n<a href=\"http://imgur.com/8M0kDpl\"><img src=\"http://i.imgur.com/8M0kDpl.png\" title=\"source: imgur.com\" /></a>\r\n\r\n### Bank C\r\n<a href=\"http://imgur.com/1HXa5Fg\"><img src=\"http://i.imgur.com/1HXa5Fg.png\" title=\"source: imgur.com\" /></a>\r\n\r\n### Bank D\r\n<a href=\"http://imgur.com/wp8v2m4\"><img src=\"http://i.imgur.com/wp8v2m4.png\" title=\"source: imgur.com\" /></a>\r\n\r\n# Conclusions\r\nIt is important to note that not every tweet or post speaks directly about the bank. You must remember that there are other factors. The consumer may have had a bad experience indirectly involving the bank and used that bank as a scapegoat for their anger. With that in mind, I will present numerical data from each data set. The average sentiment score of the entire data set is 62. Bank A scored slightly above that with an average score of 65 and Bank D scored significantly lower with an average score of 52. Based on the sentiment scores, Bank D's customers are less satisfied than customers of other banks; however, the conclusion that Bank D is worse than the other banks cannot be made. There could have been one day that the bank's systems went down, which would cause a large amount of negative press for the bank. I would like to further study a timeline of Bank D's tweets/posts because if there is a heavy increase of negative posts at the same time then there could have been some kind of mishap. Also, Bank D is mentioned the most out of the other bans in the data set which causes me to wonder if there was a mishap that created a lot of press for Bank D. Besides Bank D's performance, the data also shows that Facebook consistently has a larger average sentiment score than Twitter. I think that this may be caused by an age difference between users of each social media platform. According to a study done by the Pew Research Center in 2014, published Jan. 2015, 71% of adult internet users use Facebook, while only 23% of adult internet users use Twitter. I think this creates a more juvenile atmosphere for Twitter and a more mature, maybe even professional, atmosphere for Facebook. I think the atmosphere difference could cause a difference in sentiment of each post. This is why it is important to use multiple platforms when analyzing data, especially when analyzing customer service. Customer service is extremely important for all companies and using social media is a genius way to analyze how a company is performing. Social media shows raw emotion and truthful opinions because of the confidence that the anonymity a keyboard and a screen gives to users. I would like to further investigate not only a timeline of these posts, but also create word association graphs to show how these words are connected to help Bank D find it's struggling point. \r\n\r\n\r\n# Special Thanks\r\nI'd like to thank Wells Fargo for presenting this opportunity. Specifically, I'd like to thank Wells Fargo's Office of Innovation, Enterprise of Social Media, and Chief Data Office - Enterprise Data & Analytics teams for sponsoring this competition. I would also like to thank my colleagues, Katie Duchinski and Emily Berich, for their dedication and hard work on this project.\r\n\r\n# Authors and Contributors\r\n@kduchinski\r\n@emilyberich","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}